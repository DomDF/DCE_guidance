@article{DiFrancescoa,
author = {{Di Francesco}, Domenic},
journal = {TBC},
title = {{System Effects in Value of Information Analysis (in preparation)}}
}
@article{Gkantonas2021,
author = {Gkantonas, Savvas and Zabotti, Daniel and Mesquita, Leo CC and Mastorakos, Epaminodas and DeOlivera, Pedro M},
doi = {https://doi.org/10.17863/CAM.72192},
title = {{No Title}},
year = {2021}
}
@article{Jans-Singh2020,
abstract = {This paper presents the development process of a digital twin of a unique hydroponic underground farm in London, Growing Underground (GU). Growing 12x more per unit area than traditional greenhouse farming in the UK, the farm also consumes 4x more energy per unit area. Key to the ongoing operational success of this farm and similar enterprises is finding ways to minimize the energy use while maximizing crop growth by maintaining optimal growing conditions. As such, it belongs to the class of Controlled Environment Agriculture, where indoor environments are carefully controlled to maximize crop growth by using artificial lighting and smart heating, ventilation, and air conditioning systems. We tracked changing environmental conditions and crop growth across 89 different variables, through a wireless sensor network and unstructured manual records, and combined all the data into a database. We show how the digital twin can provide enhanced outputs for a bespoke site like GU, by creating inferred data fields, and show the limitations of data collection in a commercial environment. For example, we find that lighting is the dominant environmental factor for temperature and thus crop growth in this farm, and that the effects of external temperature and ventilation are confounded. We combine information learned from historical data interpretation to create a bespoke temperature forecasting model (root mean squared error < 1.3°C), using a dynamic linear model with a data-centric lighting component. Finally, we present how the forecasting model can be integrated into the digital twin to provide feedback to the farmers for decision-making assistance.},
author = {Jans-Singh, Melanie and Leeming, Kathryn and Choudhary, Ruchi and Girolami, Mark},
doi = {10.1017/dce.2020.21},
file = {:Users/ddifrancesco/Downloads/digital-twin-of-an-urban-integrated-hydroponic-farm.pdf:pdf},
issn = {26326736},
journal = {Data-Centric Engineering},
keywords = {Data-centric model,hourly forecasting,hydroponic farm,underground farm,urban-integrated farm},
number = {2},
title = {{Digital twin of an urban-integrated hydroponic farm}},
volume = {1},
year = {2020}
}
@inproceedings{Hopkins2002,
abstract = {.In pipeline engineering you don't need anything at all! You don't need qualified engineers, you don't need quality systems, you don't need risk management, you don't need safety audits, you don't need inspections, you don't need training. You don't need anything! Until something happens... then you need everything.... Got the message? Pipeline integrity means pipeline safety and security, and it is currently a ‘hot' topic following numerous high profile and tragic failures. The integrity of a pipeline is critically dependent on its engineering. and managing risks. This paper explains the increasing importance of training and shows why it is essential that a pipeline's management and engineers understand many aspects of pipeline engineering to be able to understand a pipeline's integrity needs. Additionally, the paper outlines the integrity training needs for pipeline engineers.},
address = {Tempe, Arizona},
author = {Hopkins, Phil},
booktitle = {Western Regional Gas Conference},
file = {:Users/ddifrancesco/Library/CloudStorage/OneDrive-TheAlanTuringInstitute/References/training-engineers.pdf:pdf},
title = {{Training Engineers in Pipeline Integrity}},
year = {2002}
}
@book{JCSS2001,
author = {JCSS},
institution = {Joint Committee on Structural Safety},
isbn = {978-3-909386-79-6},
title = {{Probabilistic Model Code}},
url = {https://www.jcss-lc.org/jcss-probabilistic-model-code/},
year = {2001}
}
@techreport{Martin2019,
abstract = {This document has been generated by the Nuclear Structural Integrity Probabilistics Working Group, which is formed from leading structural integrity specialists from industry and academia. The Working Group has developed this document to promote further discussion, and it does not represent the corporate views of, nor has it been endorsed by, any of the contributors' parent organisations. The document is not intended to be a ‘Code' or ‘Standard' but describes principles and provides guidance on approaches which may bring benefit. The document has not been endorsed by any public body or by the nuclear regulatory community. Users are advised to discuss with their regulators before attempting to apply the guidance provided.},
author = {Martin, Mike and Marshall, Rob},
file = {:Users/ddifrancesco/Downloads/nuclear_SI_probabilistic_working_principlesat.pdf:pdf},
institution = {The Nuclear Structural Integrity Probabilistics Working Group},
number = {April},
pages = {1--91},
title = {{Nuclear Structural Integrity Probabilistic Working Principles}},
url = {https://www.fesi.org.uk/news/nuclear-structural-integrity-probabilistics-working-principles/},
year = {2019}
}
@article{DiFrancesco2022,
abstract = {Failure assessment diagrams are an integral component of asset integrity management in a variety of industrial sectors. They allow for the assessment of the significance of cracks in structures, between the domains of brittle fracture and plastic collapse. Numerical modelling, or empirically determined assessment lines across this continuum, are the basis of the guidance in current industrial standards. However, as the requirement of such assessments progresses from demonstrating safety to optimising resilience and resource allocation, it will become necessary to compute more informative (probabilistic) estimates, which are compatible with decision analysis. In this paper, Bayesian regression models are proposed as a suitable method of quantifying (aleatory and epistemic) uncertainty in the limit state on a failure assessment diagram. The data considered in this study consists of laboratory tests completed on wide plate fracture specimens. This work is intended to address the inconsistencies in current editions of industrial standards, and limitations (regarding flexibility and application) of existing scientific literature on the topic. Potential applications are discussed, including the use of fracture mechanics in meaningful reliability analysis, quantitative risk management, and optimising experimental design for future material tests.},
author = {{Di Francesco}, Domenic and Girolami, Mark and Duncan, Andrew B. and Chryssanthopoulos, Marios},
doi = {10.1016/j.strusafe.2022.102262},
issn = {01674730},
journal = {Structural Safety},
keywords = {Data-centric engineering,Experimental design,Fracture mechanics,Gaussian process regression,Structural reliability},
month = {nov},
publisher = {Elsevier B.V.},
title = {{A probabilistic model for quantifying uncertainty in the failure assessment diagram}},
volume = {99},
year = {2022}
}
@article{Ghahramani2015,
abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
author = {Ghahramani, Zoubin},
doi = {10.1038/nature14541},
issn = {14764687},
journal = {Nature},
number = {7553},
pages = {452--459},
pmid = {26017444},
title = {{Probabilistic machine learning and artificial intelligence}},
volume = {521},
year = {2015}
}
@article{DiFrancesco,
author = {{Di Francesco}, Domenic and Chryssanthopoulos, Marios and Faber, Michael Havbro and Bharadwaj, Ujjwal},
doi = {10.1017/dce.2021.18},
journal = {Data Centric Engineering},
number = {18},
title = {{Decision-Theoretic Inspection Planning Using Imperfect and Incomplete Data}},
volume = {2},
year = {2021}
}
@techreport{Gelman2020a,
abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
archivePrefix = {arXiv},
arxivId = {2011.01808},
author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"{u}}rkner, Paul-Christian and Modr{\'{a}}k, Martin},
eprint = {2011.01808},
title = {{Bayesian Workflow}},
year = {2020}
}
@article{Straub2017,
abstract = {The concept of value of information (VoI) enables quantification of the benefits provided by structural health monitoring (SHM) systems – in principle. Its implementation is challenging, as it requires an explicit modelling of the structural system's life cycle, in particular of the decisions that are taken based on the SHM information. In this paper, we approach the VoI analysis through an influence diagram (ID), which supports the modelling process. We provide a simple example for illustration and discuss challenges associated with real-life implementation.},
author = {Straub, Daniel and Chatzi, Eleni and Bismut, Elizabeth and Courage, Wim and Dohler, Michael and Faber, Michael Havbro and Kohler, Jochen and Lombaert, Geert and Omenzetter, Piotr and Pozzi, Matteo and Th, Sebastian and VAL, Dimitri V and WENZEL, Helmut and Zonta, Daniele and Dohler, Michael and Faber, Michael Havbro and Kohler, Jochen and Lombaert, Geert and Omenzetter, Piotr and Pozzi, Matteo and Thons, Sebastian and Va, Dimitri V and Wen, Helmut and Zonta, Daniele},
isbn = {9783903024281},
journal = {ICOSSAR – 12th International Conference on Structural Safety & Reliability},
number = {August},
pages = {13 p},
title = {{Value of information: A roadmap to quantifying the benefit of structural health monitoring}},
url = {https://hal.inria.fr/hal-01577257/document},
year = {2017}
}
@book{Gelman2020,
author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
doi = {10.1017/9781139161879},
edition = {First},
editor = {Alvarez, R. Michael and Beck, Nathaniel L. and Morgan, Stephen L. and Wu, Lawrence L.},
isbn = {978-1-107-67651-0},
pages = {534},
publisher = {Cambridge University Press},
title = {{Regression and Other Stories}},
year = {2020}
}
@phdthesis{Rainforth2017,
abstract = {Imagine a world where computational simulations can be inverted as easily as running them forwards, where data can be used to refine models automatically, and where the only expertise one needs to carry out powerful statistical analysis is a basic proficiency in scientific coding. Creating such a world is the ambitious long-term aim of probabilistic programming. The bottleneck for improving the probabilistic models, or simulators, used throughout the quantitative sciences, is often not an ability to devise better models conceptually, but a lack of expertise, time, or resources to realize such innovations. Probabilistic programming systems (PPSs) help alleviate this bottleneck by providing an expressive and accessible modeling framework, then automating the required computation to draw inferences from the model, for example finding the model parameters likely to give rise to a certain output. By decoupling model specification and inference, PPSs streamline the process of developing and drawing inferences from new models, while opening up powerful statistical methods to non-experts. Many systems further provide the flexibility to write new and exciting models which would be hard, or even impossible, to convey using conventional statistical frameworks. The central goal of this thesis is to improve and extend PPSs. In particular, we will make advancements to the underlying inference engines and increase the range of problems which can be tackled. For example, we will extend PPSs to a mixed inference-optimization framework, thereby providing automation of tasks such as model learning and engineering design. Meanwhile, we make inroads into constructing systems for automating adaptive sequential design problems, providing potential applications across the sciences. Furthermore, the contributions of the work reach far beyond probabilistic programming, as achieving our goal will require us to make advancements in a number of related fields such as particle Markov chain Monte Carlo methods, Bayesian optimization, and Monte Carlo fundamentals.},
author = {Rainforth, Tom},
pages = {1 -- 247},
school = {University of Oxford},
title = {{Automating Inference, Learning, and Design using Probabilistic Programming}},
url = {http://www.robots.ox.ac.uk/$\sim$twgr/assets/pdf/rainforth2017thesis.pdf},
year = {2017}
}
@book{Berger1985,
address = {New York},
author = {Berger, James O.},
edition = {2nd},
isbn = {0-387-96098-8},
pages = {617},
publisher = {Springer},
title = {{Statistical Decision Theory and Bayesian Analysis}},
year = {1985}
}
@book{Jaynes2003,
address = {New York},
author = {Jaynes, Edwin T},
edition = {First},
isbn = {978-0-511-06589-7},
publisher = {Cambridge University Press},
title = {{Probability Theory: The Logic of Science}},
year = {2003}
}
@phdthesis{Harding2008,
author = {Harding, Catherine Ann},
school = {University of Melbourne},
title = {{Methods for Assessment of Probability of Detection for Nondestructive Inspections}},
year = {2008}
}
@book{Benjamin2014c,
abstract = {This text covers the development of decision theory and related applications of probability. Extensive examples and illustrations cultivate students' appreciation for applications, including strength of materials, soil mechanics, construction planning, and water-resource design. Emphasis on fundamentals makes the material accessible to students trained in classical statistics and provides a brief introduction to probability. 1970 edition.},
address = {Mineola, New York},
author = {Benjamin, Jack R. and Cornell, C. Allin},
edition = {Dover},
isbn = {10: 0-486-78072-4},
publisher = {Dover Publications, Inc.},
title = {{Probability, Statistics and Decision for Civil Engineers}},
year = {2014}
}
@book{McElreath2019,
abstract = {The book teaches generalized linear multilevel modeling (GLMMs) from a Bayesian perspective, relying on a simple logical interpretation of Bayesian probability and maximum entropy. The book covers the basics of regression through multilevel models, as well as touching on measurement error, missing data, and Gaussian process models for spatial and network autocorrelation. This is not a traditional mathematical statistics book. Instead the approach is computational, using complete R code examples, aimed at developing skilled and skeptical scientists. Theory is explained through simulation exercises, using R code. And modeling examples are fully worked, with R code displayed within the main text. Mathematical depth is given in optional "overthinking" boxes throughout.},
author = {McElreath, Richard},
edition = {2nd},
isbn = {978-0-367-13991-9},
publisher = {Chapman & Hall / CRC},
title = {{Statistical Rethinking : A Bayesian Course with Examples in R and Stan}},
url = {https://xcelab.net/rm/statistical-rethinking/},
year = {2020}
}
@book{Melchers2018,
author = {Melchers, Robert E. and Beck, Andr{\'{e}} T.},
edition = {Third Edit},
isbn = {9781119265993},
publisher = {John Wiley & Sons Ltd},
title = {{Structural Reliability Analysis and Prediction}},
year = {2018}
}
@article{Palmer2012,
abstract = {Pipeline reliability analysis appears at first sight to be related to the probability analysis to which everyone is accustomed. In reality, it is substantially different, and the numerical failure probabilities it arrives at are nominal and unrelated to real probabilities. This matters because it misleads the engineer and the wider community, and because it may lead to an illusion of confidence and safety that the analysis and the underlying data do not begin to justify. This paper discusses the problem, and how codes might be better written.},
author = {Palmer, Andrew},
journal = {Journal of Pipeline Engineering},
number = {4},
pages = {269--273},
title = {10**-6 and all that: what do failure probabilities mean?},
volume = {12},
year = {2012}
}
@book{BSI2015,
address = {London},
author = {BSI},
booktitle = {BSI Standards Publication},
institution = {British Standards Institution},
isbn = {978 0 580 52086 0},
title = {{BS 7910:2019 Guide to methods for assessing the acceptability of flaws in metallic structures}},
year = {2019}
}
@book{Jordaan2005,
abstract = {To better understand the core concepts of probability and to see how they affect real-world decisions about design and system performance, engineers and scientists might want to ask themselves the following questions: {\textperiodcentered} What exactly is meant by probability? {\textperiodcentered} What is the precise definition of the 100-year load and how is it calculated? {\textperiodcentered} What is an “extremal” probability distribution? {\textperiodcentered} What is the Bayesian approach? {\textperiodcentered} How is utility defined? {\textperiodcentered} How do games fit into probability theory? {\textperiodcentered} What is entropy? {\textperiodcentered} How do I apply these ideas in risk analysis? Starting from the most basic assumptions, this book develops a coherent theory of probability and broadens it into applications in decision theory, design, and risk analysis. This book is written for engineers and scientists interested in probability and risk. It can be used by undergraduates, graduate students, or practicing engineers.},
author = {Jordaan, Ian},
booktitle = {Decisions Under Uncertainty: Probabilistic Analysis for Engineering Decisions},
doi = {10.1017/CBO9780511804861},
isbn = {0 521 78277 5},
publisher = {Cambridge University Press},
title = {{Decisions under Uncertainty}},
year = {2005}
}
@article{Olsson2003,
abstract = {Latin hypercube sampling is suggested as a tool to improve the efficiency of different importance sampling methods for structural reliability analysis. In simple importance sampling, where the sampling centre is moved from the origin to the design point, standard Monte Carlo sampling can be replaced by Latin hypercube sampling. The efficiency improvement is then highly dependent on the choice of sampling directions. Different versions of Latin hypercube sampling are also successfully employed to improve the more efficient axis orthogonal importance sampling method. By means of different numerical examples, it is shown that more than 50% of the computer effort can be saved by using Latin hypercubes instead of simple Monte Carlo in importance sampling. The exact savings, however, are dependent on details in the use of Latin hypercubes and on the shape of the failure surfaces of the problems. {\textcopyright} 2002 Elsevier Science Ltd. All rights reserved.},
author = {Olsson, A. and Sandberg, G. and Dahlblom, O.},
doi = {10.1016/S0167-4730(02)00039-5},
isbn = {4646222442},
issn = {01674730},
journal = {Structural Safety},
keywords = {Axis orthogonal,Directional sampling,FORM,Importance sampling,Latin hypercube sampling,Reliability},
title = {{On Latin hypercube sampling for structural reliability analysis}},
year = {2003}
}
@book{Faber2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Faber, Michael Havbro},
booktitle = {Topics in Safety, Risk, Reliability and Quality},
doi = {10.1007/978-94-007-4056-3},
editor = {Gheorghe, Adrian V.},
eprint = {arXiv:1011.1669v3},
isbn = {978-94-007-4055-6},
issn = {1566-0443},
pages = {160},
publisher = {Springer},
title = {{Statistics and Probability Theory In Pursuit of Engineering Decision Support}},
volume = {18},
year = {2012}
}
@book{Joe2015,
abstract = {Dependence Modeling with Copulas},
author = {Joe, Harry},
booktitle = {CRC Press},
doi = {10.1007/0-387-28678-0},
isbn = {9781466583221},
pages = {479},
title = {{Dependence Modeling with Copulas}},
year = {2015}
}
@techreport{Health&SafetyExecutive2001,
abstract = {This report concerns the uses and abuses of probabilistic methods in structural integrity, in particular the design and assessment of pressure systems. Probabilistic methods are now used widely in the assessment and design of structures in many industries. For a number of years these methods have been applied to offshore pipelines and, following work by a number of companies in this country and worldwide, they are being used in the design and reassessment/requalification of major gas trunklines onshore.\nAlthough probabilistic methods have been available for a number of years and are widely used, there is still a great deal of confusion which arises from vague language, ill-defined and inconsistent terminology, and misinterpretation often present in published material on the topic. This is perhaps the main reason for misuse in some applications of the methods. The report aims to define the terms more clearly, and to outline the basic principles of the approach.\nThe report reviews the development of structural design methods from the earliest building methods, through limit state and partial factor design methods, to probabilistic analysis, and considers their applicability to assessing the integrity of pressure systems. Methods of probabilistic analysis are discussed and key references are identified for further information. The uses of risk and reliability analysis are also discussed, and many of the concerns that are often expressed with the use of reliability and risk analysis methods are examined.\nGuidelines are presented for regulators and industry to assist in assessing work that incorporates risk and reliability-based analysis and arguments. The guidelines may also be of use to consultants in how to undertake and present risk and reliability analysis.\nThis report and the work it describes was funded by the Health and Safety Executive (HSE). Its contents, including any opinions and/or conclusions expressed, are those of the author(s) alone and do not necessarily reflect HSE policy.},
author = {{Health & Safety Executive}},
file = {:Users/ddifrancesco/Library/Application Support/Mendeley Desktop/Downloaded/Health & Safety Executive - 2001 - Probabilistic methods Uses and abuses in structural integrity.pdf:pdf},
isbn = {071762238X},
number = {398/2001},
pages = {1--226},
title = {{Probabilistic methods: Uses and abuses in structural integrity}},
year = {2001}
}
