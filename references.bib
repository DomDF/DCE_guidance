@article{Gordon2014,
abstract = {Probabilistic programs are usual functional or imperative programs with two added constructs: (1) the ability to draw values at random from distributions, and (2) the ability to condition values of variables in a program via observations. Models from diverse application areas such as computer vision, coding theory, cryptographic protocols, biology and reliability analysis can be written as probabilistic programs. Probabilistic inference is the problem of computing an explicit representation of the probability distribution implicitly specified by a probabilistic program. Depending on the application, the desired output from inference may vary-we may want to estimate the expected value of some function f with respect to the distribution, or the mode of the distribution, or simply a set of samples drawn from the distribution. In this paper, we describe connections this research area called \Probabilistic Programming" has with programming languages and software engineering, and this includes language design, and the static and dynamic analysis of programs. We survey current state of the art and speculate on promising directions for future research.},
author = {Gordon, Andrew D. and Henzinger, Thomas A. and Nori, Aditya V. and Rajamani, Sriram K.},
doi = {10.1145/2593882.2593900},
isbn = {9781450328654},
journal = {Future of Software Engineering, FOSE 2014 - Proceedings},
keywords = {Machine learning,Probabilistic programming,Program analysis},
pages = {167--181},
title = {{Probabilistic programming}},
year = {2014}
}
@article{Carpenter2017,
abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
doi = {10.18637/jss.v076.i01},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {1},
pages = {1----32},
title = {{Stan : A Probabilistic Programming Language}},
url = {http://www.jstatsoft.org/v76/i01/},
volume = {76},
year = {2017}
}
@article{Ghahramani2015,
abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
author = {Ghahramani, Zoubin},
doi = {10.1038/nature14541},
issn = {14764687},
journal = {Nature},
number = {7553},
pages = {452--459},
pmid = {26017444},
title = {{Probabilistic machine learning and artificial intelligence}},
volume = {521},
year = {2015}
}
@phdthesis{Rainforth2017,
abstract = {Imagine a world where computational simulations can be inverted as easily as running them forwards, where data can be used to refine models automatically, and where the only expertise one needs to carry out powerful statistical analysis is a basic proficiency in scientific coding. Creating such a world is the ambitious long-term aim of probabilistic programming. The bottleneck for improving the probabilistic models, or simulators, used throughout the quantitative sciences, is often not an ability to devise better models conceptually, but a lack of expertise, time, or resources to realize such innovations. Probabilistic programming systems (PPSs) help alleviate this bottleneck by providing an expressive and accessible modeling framework, then automating the required computation to draw inferences from the model, for example finding the model parameters likely to give rise to a certain output. By decoupling model specification and inference, PPSs streamline the process of developing and drawing inferences from new models, while opening up powerful statistical methods to non-experts. Many systems further provide the flexibility to write new and exciting models which would be hard, or even impossible, to convey using conventional statistical frameworks. The central goal of this thesis is to improve and extend PPSs. In particular, we will make advancements to the underlying inference engines and increase the range of problems which can be tackled. For example, we will extend PPSs to a mixed inference-optimization framework, thereby providing automation of tasks such as model learning and engineering design. Meanwhile, we make inroads into constructing systems for automating adaptive sequential design problems, providing potential applications across the sciences. Furthermore, the contributions of the work reach far beyond probabilistic programming, as achieving our goal will require us to make advancements in a number of related fields such as particle Markov chain Monte Carlo methods, Bayesian optimization, and Monte Carlo fundamentals.},
author = {Rainforth, Tom},
pages = {1 -- 247},
school = {University of Oxford},
title = {{Automating Inference, Learning, and Design using Probabilistic Programming}},
url = {http://www.robots.ox.ac.uk/$\sim$twgr/assets/pdf/rainforth2017thesis.pdf},
year = {2017}
}
@inproceedings{Kucukelbir2015,
abstract = {Variational inference is a scalable technique for approximate Bayesian inference. Deriving variational inference algorithms requires tedious model-specific calculations; this makes it difficult for non-experts to use. We propose an automatic variational inference algorithm, automatic differentiation variational inference (ADVI); we implement it in Stan (code available), a probabilistic programming system. In ADVI the user provides a Bayesian model and a dataset, nothing else. We make no conjugacy assumptions and support a broad class of models. The algorithm automatically determines an appropriate variational family and optimizes the variational objective. We compare ADVI to MCMC sampling across hierarchical generalized linear models, nonconjugate matrix factorization, and a mixture model. We train the mixture model on a quarter million images. With ADVI we can use variational inference on any model we write in Stan.},
archivePrefix = {arXiv},
arxivId = {1506.03431},
author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David M.},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1506.03431},
file = {:Users/ddifrancesco/OneDrive - The Alan Turing Institute/Bayes_FAD/ADVI_neurips2015.pdf:pdf},
issn = {10495258},
pages = {568--576},
title = {{Automatic variational inference in Stan}},
year = {2015}
}
@techreport{Gelman2020a,
abstract = {The Bayesian approach to data analysis provides a powerful way to handle uncertainty in all observations, model parameters, and model structure using probability theory. Probabilistic programming languages make it easier to specify and fit Bayesian models, but this still leaves us with many options regarding constructing, evaluating, and using these models, along with many remaining challenges in computation. Using Bayesian inference to solve real-world problems requires not only statistical skills, subject matter knowledge, and programming, but also awareness of the decisions made in the process of data analysis. All of these aspects can be understood as part of a tangled workflow of applied Bayesian statistics. Beyond inference, the workflow also includes iterative model building, model checking, validation and troubleshooting of computational problems, model understanding, and model comparison. We review all these aspects of workflow in the context of several examples, keeping in mind that in practice we will be fitting many models for any given problem, even if only a subset of them will ultimately be relevant for our conclusions.},
archivePrefix = {arXiv},
arxivId = {2011.01808},
author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and B{\"{u}}rkner, Paul-Christian and Modr{\'{a}}k, Martin},
eprint = {2011.01808},
title = {{Bayesian Workflow}},
year = {2020}
}
@article{Chaloner1995,
abstract = {This paper reviews the literature on Bayesian experimental design. A unified view of this topic is presented, based on a decision- theoretic approach. This framework casts criteria from the Bayesian lit- erature of design as part of a single coherent approach. The decision- theoretic structure incorporates both linear and nonlinear design problems and it suggests possible new directions to the experimental design problem, motivated by the use of new utility functions. We show that, in some special cases of linear design problems, Bayesian solutions change in a sensible way when the prior distribution and the utility func- tion are modified to allow for the specific structure of the experiment. The decision-theoretic approach also gives a mathematical justification for r selecting the appropriate optimality criterion.},
author = {Chaloner, Kathryn and Verdinelli, Isabella},
journal = {Statistical Science},
keywords = {Decision theory,hierarchical linear models,lo- gistic regression,nonlinear design,nonlinear models,optimal design,optimality criteria,utility functions},
number = {3},
pages = {273--304},
title = {{Bayesian Experimental Design : A Review}},
url = {https://www.jstor.org/stable/2246015%0A},
volume = {10},
year = {1995}
}
@book{Gelman2014,
abstract = {Now in its third edition, this classic book is widely considered the leading text on Bayesian methods, lauded for its accessible, practical approach to analyzing data and solving research problems. Bayesian Data Analysis, Third Edition continues to take an applied approach to analysis using up-to-date Bayesian methods. The authors—all leaders in the statistics community—introduce basic concepts from a data-analytic perspective before presenting advanced methods. Throughout the text, numerous worked examples drawn from real applications and research emphasize the use of Bayesian inference in practice. New to the Third Edition Four new chapters on nonparametric modeling Coverage of weakly informative priors and boundary-avoiding priors Updated discussion of cross-validation and predictive information criteria Improved convergence monitoring and effective sample size calculations for iterative simulation Presentations of Hamiltonian Monte Carlo, variational Bayes, and expectation propagation New and revised software code The book can be used in three different ways. For undergraduate students, it introduces Bayesian inference starting from first principles. For graduate students, the text presents effective current approaches to Bayesian modeling and computation in statistics and related fields. For researchers, it provides an assortment of Bayesian methods in applied statistics. Additional materials, including data sets used in the examples, solutions to selected exercises, and software instructions, are available on the book's web page.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Gelman, Andrew and Carlin, John B B and Stern, Hal S S and Rubin, Donald B B},
doi = {10.1007/s13398-014-0173-7.2},
edition = {3rd},
editor = {Dominici, Francesca and Faraway, Julian J. and Tanner, Martin and Zidek, Jim},
eprint = {arXiv:1011.1669v3},
isbn = {9781439840955},
issn = {1467-9280},
pages = {675},
pmid = {25052830},
publisher = {Chapman & Hall / CRC},
title = {{Bayesian Data Analysis}},
year = {2014}
}
@inproceedings{Foster2019,
abstract = {Bayesian optimal experimental design (BOED) is a principled framework for making efficient use of limited experimental resources. Unfortunately, its applicability is hampered by the difficulty of obtaining accurate estimates of the expected information gain (EIG) of an experiment. To address this, we introduce several classes of fast EIG estimators by building on ideas from amortized variational inference. We show theoretically and empirically that these estimators can provide significant gains in speed and accuracy over previous approaches. We further demonstrate the practicality of our approach on a number of end-to-end experiments.},
archivePrefix = {arXiv},
arxivId = {1903.05480},
author = {Foster, Adam and Jankowiak, Martin and Bingham, Eli and Horsfall, Paul and Teh, Yee Whye and Rainforth, Tom and Goodman, Noah},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1903.05480},
issn = {10495258},
title = {{Variational Bayesian optimal experimental design}},
url = {https://papers.nips.cc/paper/2019/file/d55cbf210f175f4a37916eafe6c04f0d-Paper.pdf},
volume = {32},
year = {2019}
}
@book{Lambert2018,
address = {London},
author = {Lambert, Ben},
edition = {First},
editor = {Seaman, Jai},
isbn = {978-1-4739-1635-7},
publisher = {SAGE Publications Ltd.},
title = {{A Student's Guide to Bayesian Statistics}},
year = {2018}
}
@book{McElreath2019,
abstract = {The book teaches generalized linear multilevel modeling (GLMMs) from a Bayesian perspective, relying on a simple logical interpretation of Bayesian probability and maximum entropy. The book covers the basics of regression through multilevel models, as well as touching on measurement error, missing data, and Gaussian process models for spatial and network autocorrelation. This is not a traditional mathematical statistics book. Instead the approach is computational, using complete R code examples, aimed at developing skilled and skeptical scientists. Theory is explained through simulation exercises, using R code. And modeling examples are fully worked, with R code displayed within the main text. Mathematical depth is given in optional "overthinking" boxes throughout.},
author = {McElreath, Richard},
edition = {2nd},
isbn = {978-0-367-13991-9},
publisher = {Chapman & Hall / CRC},
title = {{Statistical Rethinking : A Bayesian Course with Examples in R and Stan}},
url = {https://xcelab.net/rm/statistical-rethinking/},
year = {2020}
}
@book{Berger1985,
address = {New York},
author = {Berger, James O.},
edition = {2nd},
isbn = {0-387-96098-8},
pages = {617},
publisher = {Springer},
title = {{Statistical Decision Theory and Bayesian Analysis}},
year = {1985}
}
@book{Kruschke2014,
abstract = {There is an explosion of interest in Bayesian statistics, primarily because recently created computational methods have finally made Bayesian analysis obtainable to a wide audience. Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan provides an accessible approach to Bayesian data analysis, as material is explained clearly with concrete examples. The book begins with the basics, including essential concepts of probability and random sampling, and gradually progresses to advanced hierarchical modeling methods for realistic data. Included are step-by-step instructions on how to conduct Bayesian data analyses in the popular and free software R and WinBugs. This book is intended for first-year graduate students or advanced undergraduates. It provides a bridge between undergraduate training and modern Bayesian methods for data analysis, which is becoming the accepted research standard. Knowledge of algebra and basic calculus is a prerequisite.},
author = {Kruschke, John K.},
booktitle = {Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan, Second Edition},
doi = {10.1016/B978-0-12-405888-0.09999-2},
edition = {2},
isbn = {9780124058880},
pages = {1--759},
publisher = {Elsevier Inc.},
title = {{Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan, second edition}},
year = {2014}
}
@book{Box1992,
author = {Box, George and Tiao, George C.},
edition = {Wiley Clas},
isbn = {978-81-265-4706-7},
publisher = {Wiley},
title = {{Bayesian Inference in Statistical Analysis}},
year = {1992}
}
@article{Carpenter2017,
abstract = {Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
doi = {10.18637/jss.v076.i01},
issn = {1548-7660},
journal = {Journal of Statistical Software},
number = {1},
pages = {1----32},
title = {{Stan : A Probabilistic Programming Language}},
url = {http://www.jstatsoft.org/v76/i01/},
volume = {76},
year = {2017}
}
@book{Raiffa1961,
address = {Boston},
author = {Raiffa, Howard and Schlaifer, Robert},
booktitle = {Journal of Chemical Information and Modeling},
file = {:Users/ddifrancesco/Library/Application Support/Mendeley Desktop/Downloaded/Raiffa, Schlaifer - 1961 - Applied Statistical Decision Theory.pdf:pdf},
isbn = {0-87584-017-5},
publisher = {Graduate School of Business Administration Harvard University},
title = {{Applied Statistical Decision Theory}},
year = {1961}
}
@article{Lau2018,
abstract = {We explore the role of statistics for Big Data analysis arising from the emerging field of Data-Centric Engineering. Using examples related to sensor-instrumented bridges, we highlight a number of issues and challenges. These are broadly categorised as relating to uncertainty, latent-structure modelling, and the synthesis of statistical models and abstract physical models.},
author = {Lau, F. Din-Houn and Adams, Niall M. and Girolami, Mark A. and Butler, Liam J. and Elshafie, Mohammed Z.E.B.},
doi = {10.1016/J.SPL.2018.02.035},
file = {:Users/ddifrancesco/Library/Application Support/Mendeley Desktop/Downloaded/Lau et al. - 2018 - The role of statistics in data-centric engineering.pdf:pdf},
issn = {0167-7152},
journal = {Statistics & Probability Letters},
month = {may},
pages = {58--62},
publisher = {North-Holland},
title = {{The role of statistics in data-centric engineering}},
volume = {136},
year = {2018}
}
@book{Jaynes2003,
address = {New York},
author = {Jaynes, Edwin T},
edition = {First},
isbn = {978-0-511-06589-7},
publisher = {Cambridge University Press},
title = {{Probability Theory: The Logic of Science}},
year = {2003}
}
@inproceedings{Johnston2017,
abstract = {Statistical fatigue data of various materials at different conditions are increasingly required by engineers for reliabillity design purposes. The present paper describes a new way of analysing fatigue data, which is promising to take out maximum statistical informations from ordinary small sample test results. The method is essentially based on examining the distribution in relative strength deviation of individual test data, evaluated against the mean strength at the considered life level. The Probit analysis using weighting coefficients, under the condition of a known variation coefficient, is quite suitable for the mean strength estimation. Starting with a provisional variation coefficient, the test data could be transformed into a set of relative deviation values in strength; a real variation coefficient is then evaluated according to the theory of multiply censored order stati-stics. The proposed method could also be used most effectively in analysing a pile of small sample data sets plotted on a common strength scale, e.g., the ratio against tensile strength of the material.},
address = {Trondheim, Norway},
author = {Johnston, Carol},
booktitle = {ASME 2017 36th International Conference on Ocean, Offshore and Arctic Engineering, OMAE2017},
doi = {10.2472/jsms.29.24},
isbn = {9780791857687},
title = {{Statistical Analysis of Fatigue Test Data}},
url = {https://www.twi-global.com/technical-knowledge/published-papers/statistical-analysis-of-fatigue-test-data%0A},
year = {2017}
}
@techreport{JointCommitteeonStructuralSafety2009,
author = {{Joint Committee on Structural Safety}},
booktitle = {Joint Committee on Structural Safety Probabilistic Model Code},
doi = {10.1016/B978-0-444-53427-9.00027-0},
file = {:Users/ddifrancesco/Library/Application Support/Mendeley Desktop/Downloaded/Joint Committee on Structural Safety - 2009 - Model Uncertainty.pdf:pdf},
institution = {JCSS},
isbn = {978-0-444-53427-9},
issn = {1742-3422},
pages = {478},
pmid = {24174412},
title = {{Model Uncertainty}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B9780444534279000270},
year = {2009}
}
@book{Jordaan2005,
abstract = {To better understand the core concepts of probability and to see how they affect real-world decisions about design and system performance, engineers and scientists might want to ask themselves the following questions: {\textperiodcentered} What exactly is meant by probability? {\textperiodcentered} What is the precise definition of the 100-year load and how is it calculated? {\textperiodcentered} What is an “extremal” probability distribution? {\textperiodcentered} What is the Bayesian approach? {\textperiodcentered} How is utility defined? {\textperiodcentered} How do games fit into probability theory? {\textperiodcentered} What is entropy? {\textperiodcentered} How do I apply these ideas in risk analysis? Starting from the most basic assumptions, this book develops a coherent theory of probability and broadens it into applications in decision theory, design, and risk analysis. This book is written for engineers and scientists interested in probability and risk. It can be used by undergraduates, graduate students, or practicing engineers.},
author = {Jordaan, Ian},
booktitle = {Decisions Under Uncertainty: Probabilistic Analysis for Engineering Decisions},
doi = {10.1017/CBO9780511804861},
isbn = {0 521 78277 5},
publisher = {Cambridge University Press},
title = {{Decisions under Uncertainty}},
year = {2005}
}
@book{Gelman2020,
author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
doi = {10.1017/9781139161879},
edition = {First},
editor = {Alvarez, R. Michael and Beck, Nathaniel L. and Morgan, Stephen L. and Wu, Lawrence L.},
isbn = {978-1-107-67651-0},
pages = {534},
publisher = {Cambridge University Press},
title = {{Regression and Other Stories}},
year = {2020}
}
@book{Ang2007,
abstract = {2nd ed. Revised edition of: Probability concepts in engineering planning and design. 1975-c1984. Roles of probability and statistics in engineering -- Fundamentals of probability models -- Analytical models of random phenomena -- Functions of random variables -- Computer-based numerical and simulation methods in probability -- Statistical inferences from observational data -- Determination of probability distribution models -- Regression and correlation analyses -- The Bayesian approach -- Elements of quality assurance and acceptance sampling.},
address = {New York},
author = {Ang, AHS and Tang, WH},
edition = {Second Edi},
isbn = {978-0471720645},
pmid = {12664600},
publisher = {Wiley},
title = {{Probability Concepts in Engineering}},
year = {2007}
}
@book{Anderson2005,
author = {Anderson, Ted L.},
edition = {Third},
isbn = {978-1-4200-5821-5},
publisher = {Taylor & Francis},
title = {{Fracture Mechanics Fundamentals and Applications}},
year = {2005}
}
@book{Benjamin2014c,
abstract = {This text covers the development of decision theory and related applications of probability. Extensive examples and illustrations cultivate students' appreciation for applications, including strength of materials, soil mechanics, construction planning, and water-resource design. Emphasis on fundamentals makes the material accessible to students trained in classical statistics and provides a brief introduction to probability. 1970 edition.},
address = {Mineola, New York},
author = {Benjamin, Jack R. and Cornell, C. Allin},
edition = {Dover},
isbn = {10: 0-486-78072-4},
publisher = {Dover Publications, Inc.},
title = {{Probability, Statistics and Decision for Civil Engineers}},
year = {2014}
}
@book{Melchers2018,
author = {Melchers, Robert E. and Beck, Andr{\'{e}} T.},
edition = {Third Edit},
isbn = {9781119265993},
publisher = {John Wiley & Sons Ltd},
title = {{Structural Reliability Analysis and Prediction}},
year = {2018}
}

@article{copula,
	title = {copula: Multivariate Dependence with Copulas},
	author = {Hofert, Marius and Kojadinovic, Ivan and Maechler, Martin and Yan, Jun},
	year = {2022},
	date = {2022},
	url = {https://CRAN.R-project.org/package=copula}
}
