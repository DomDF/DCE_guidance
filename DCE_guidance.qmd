---
title: "Data-Centric Engineering"
subtitle: "Guidance on the Use of Probabilistic Methods for Identifying Data Requirements"
author: "Domenic Di Francesco, PhD, CEng"
date: "June 2022"
format:
  html:
    theme: spacelab
    html-math-mathod: mathjax
    toc: true
    toc-location: right
    number-sections: true
    code-copy: true
    code-fold: true
    code-tools: true
    code-overflow: wrap
    monofont: Fira Code
    mermaid-format: js
execute:
  message: false
  warning: false
bibliography: references.bib
---

# Introduction

## Purpose of this Document

The various new methods of collecting and analysing data that are increasingly available to engineers can contribute to improvements in safety and efficiency of the built environment. However, understanding the quantity and quality required will continue to be a challenge to engineers. For instance:

 * Should a sensing systems be retrofit to an existing structure? 
 * If so, how precise and how reliable do they need to be? 
 * How can we value data from smart meters? 
 * Which combinations of available data collection options can best support decision making?

The principles of data-centric engineering are not new. Engineers have always had to rely on empirical models that are supported by tests to demonstrate that systems are reliable and safe. However, given the current availability of free, open-source software tools for data analysis and statistical inference, there is an opportunity to improve engineering workflows. The guidance presented here is intended to be pragmatic and introductory. Example problems are presented `r #(with reference to Eurocodes, Standards and Material Specifications)` alongside accompanying code implementations. There is a focus on answering meaningful questions, supporting decision making, and ensuring reproducible and reliable results.

## How to use this document

This is a computational document that includes chunks of `Python` and `R` code necessary to analyse the data, and solve the decision problems in the various examples. To achieve this, various libraries/packages have been used, and these will need to be installed and loaded for the code to run.

::: {#chunk_load_packages .panel-tabset}
## Load Python Packages

```{python}
import cmdstanpy, requests, os, tempfile
import numpy as np, pandas as pd
from scipy import stats

```

## Load R Packages

```{R}
library(tidyverse); library(fitdistrplus); library(boot)
library(cmdstanr); library(copula); library(RHugin)

```

## Load Julia Packages

```{julia}
using Random, Distributions, CSV, HTTP, DataFrames

```

:::

:::{.callout-tip collapse="true"}
## Tip: Loading packages
In `R` and `Python` packages first need to be installed. Guidance on installing packages can be found [here](https://support.rstudio.com/hc/en-us/articles/201057987-Quick-list-of-useful-R-packages) (for `R`) and [here](https://pypi.org/project/pip/) (for `Python`). 

Packages only need to be installed once (unless they are uninstalled), but then need to be loaded each time you want to make direct use of the functions or data they contain. This document will not detail the workings of each package, but such information can be found online, for example [here](https://pandas.pydata.org) is the website for the `Python` 'pandas' package.
:::

In addition, some statistical models that have been written in the probabilistic programming language `Stan` have been used. The data used in the examples, as well as the code used for each exercise can be freely downloaded from this [public Github repository](https://github.com/DomDF/DCE_guidance).

# Uncertainty in the Built Environment

Engineering standards acknowledge the presence of variability in the quantities they are dealing with, but do not always provide guidance on how to apply models of variability, particularly in supporting decision making. For example, repeated strength tests of specimens from the same material, using the same machine, in the same lab will produce differing, though hopefully similar) results. Any decision on whether a material is safe to use must therefore account for this variability somehow.

Historically, structural engineers have used deterministic approaches to perform conservative assessments. In this example a *safe* or *characteristic* value of strength may be taken, such as the minimum from a set of measurements. The premise of this approach is that, if the lowest measurement meets a requirement, then it is expected to be OK. These kind of heuristics do not tell us how many tests are needed to be confident that the lowest measurement is representative, since each new measurement provides an opportunity to find a new lowest strength. As a result, unquantified and implicit margins of conservatism are introduced, making it very difficult to find the best estimates of risk that are required to justify spending consistently and coherently.

This can be resolved by using probability to formally quantify uncertainty, and various examples of this are presented in this document. Statistical models of variability can describe how many uncertain quantities can be dependent, how uncertainty can increase when making predictions over various time frames, and how uncertainty can decrease when new data becomes available. Uncertainty quantification therefore allows for many other types of analysis, such as identifying where, when and how additional data should be collected, which is the focus of this document.

While data will generally always provide some value, provided that it is relevant, it will not always represent a good investment. There are various costs associated with collecting engineering data, including hiring/purchasing specialised measurement equipment, the storage costs of high-volume streaming data, and occasionally the risk associated with exposing personnel to hazardous environments to collect data. Justifying these costs require engineers to link data collection to the improved decision making that it facilitates. 

Without formal methods of uncertainty quantification, engineers may differ in opinion about data collection strategy. Quantitative approaches will be preferable because they are auditable. As demonstrated in the examples in this document, these approaches do not remove engineering input from this process, but rather include expert knowledge in a formal way. Unless both available data, and subject matter expertise are used to inform decisions, then some information is not being taken advantage of.

# Quantifying Uncertainty

As described in the below note, there are many appealing reasons to use probability to describe variability.

:::{.callout-note collapse="true"}
## Thought: Using Probabilities

It has been proposed that since probability is the mathematical language of randomness, it should be used to model the uncertainties that arise [@Gelman2014]. Another compelling argument is that the practical meaning of probability is intuitive. Probabilities can be assinged to possible (uncertain) outcomes in decision problems.

Understanding how to estimate these probabilities as well as how to use them allows for coherent and replicable (auditable) deicion making. The statistics presented in this document are with the intention of providing this kind of pragmatic guidance.

:::

There are many established probability distributions, and some reasonable question regarding their application in engineering calculations, include:

-   Where do distributions come from?
-   When are they useful, or unhelpful in describing variability in engineering quantities?



Uniform distribution can occur in the most ways. Mathematically it can be shown to have the maximum entropy (where information entropy can broadly considered to be a measure of uncertainty) of all possible distributions to describe this problem [@Jaynes2003, @Jordaan2005].

Other problems, with other features and constraints, have different maximum entropy solutions.

A maximum entropy distribution is therefore the least committal (contains the fewest assumptions), and this may be a justifiable argument for approximating an uncertain quantity. However, engineers will generally have enough knowledge about their systems to 

## Working with Probability Distributions

Both `Python` and `R` have many functions for evaluating, integrating, sampling, and estimating parameters from probability distributions. The below examples show how $10$ independent samples can be drawn from a normal ditribution, with a mean value of $0$ and a standard deviation of $1$.

::: panel-tabset
## Python (using SciPy)

```{python}
stats.norm.rvs(size = 10, loc = 0, scale = 1)

```

## R

```{R}
rnorm(n = 10, mean = 0, sd = 1)

```

## Julia (using Distributions, Random)

```{julia}
Normal(1, 0) |> x -> rand(10)

```

:::

Some variance reduction methods (add link to python and R lhs libraries)...., @fig-it_lhs.

```{R}
#| echo: false
#| label: fig-it_lhs
#| fig-cap: "Effect of sample size on methods of approximating a standard normal distribution"
library(lhs)

set.seed(seed = 1008); sampling_df <- tibble()

for(n_samples in c(20, 100, 1000)){

    probs <- lhs::randomLHS(n = n_samples, k = 1)
    lhs_df <- tibble(samples = qnorm(p = probs[,1]), method = "LH Sampling", n = n_samples)
    ind_df <- tibble(samples = rnorm(n = n_samples), method = "IT Sampling", n = n_samples)

    sampling_df <- bind_rows(sampling_df, lhs_df, ind_df)

}

ggplot(data = sampling_df |> 
    mutate(n = paste(n, "samples")) |>
    mutate(n = forcats::as_factor(n)), 
    mapping = aes(x = samples))+
    geom_histogram(mapping = aes(y = after_stat(x = density)), 
                  col = 'black', alpha = 1/2, bins = 15, position = position_dodge())+
    facet_grid(method ~ n)+
    stat_function(fun = dnorm, geom = 'line', mapping = aes(col = 'True function, N(0, 1)'))+
    labs(x = "", y = "Probability density")+
    ggthemes::theme_base(base_family = "Atkinson Hyperlegible", base_size = 11)+
    theme(legend.title = element_blank(), legend.position = 'top', plot.background = element_rect(colour = NA))

```

Sampling from distributions converts mathematically challenging statistical problems to simpler data analysis problems. Consider the basic problem of structural reliability, where an engineer is tasked with identifying the probability of an uncertain load exceeding a components uncertain resistance to that load.

Probability defined by the convolution integral.

Solution can also be obtained by counting the proportion of samples from the load model that exceed samples from the reistance model. Each set of samples can be considered as a possible outcome (realisation) from the models.


### Example: Analysis of Tensile Test Data of Steel

This example considers how to interpret a set of measurements of material strength. The data is presented in @tbl-strength_data. This data can be downloaded using the below code, which also shows the first few rows.

::: panel-tabset
## Python (using pandas)

```{python}
strength_df = pd.read_csv(filepath_or_buffer = "https://raw.githubusercontent.com/DomDF/DCE_guidance/main/data_files/strength_data.csv")

strength_df.head(n = 3)

```

## R (using readr)

```{R}
strength_df <- read_csv(file = "https://raw.githubusercontent.com/DomDF/DCE_guidance/main/data_files/strength_data.csv")

strength_df |> head(n = 3)

```
## Julia (using HTTP, CSV, DataFrames)

```{julia}
σ_df = "https://raw.githubusercontent.com/DomDF/DCE_guidance/main/data_files/strength_data.csv"

  # x -> HTTP.get(x).body |>
  # x -> CSV.File(x) |>
  # DataFrame |>
  # x -> first(x, 3)

```

:::

The results indicate some variability even though each row presents the result of the same test, using the same machine, on a tensile specimen from the same material. This variability can be attributed to:

-   **Material heterogeneity**. Manufacturing processes used to make structural steel results in local hard spots, laminations, inclusions and other anomalies that can locally influence the strength of the material. The presence of such anomalies in the microstructure of a testing specimen will influence the measured properties.

-   **Imperfect measurement data**. There is no manufacturing process that creates perfectly homogeneous steel, and there is no measurement of an engineering quantity that will tell us everything we want to know. In this example, the machine used to perform the tests will output results with some precision, which has been quantified by the manufacturers.

Shown here as a table:

```{R}
#| echo: false
#| label: tbl-strength_data
#| tbl-cap: "Tensile Test Data of Steel"

strength_df |> 
  rename('Test ID' = 'id', 'Yield Strength, MPa' = 'yield', 'Tensile Strength, MPa' = 'tensile') |> 
  knitr::kable()

```

There is a range of `r paste(signif(x = strength_df$yield |> max() - strength_df$yield |> min(), digits = 3), "MPa")`. There are many ways that this can be interpreted. Since no value was recorded less than `r paste(strength_df$yield |> min() |> floor(), "MPa")`, can it be assumed that lower yield strengths are not credible?

:::{.callout-note collapse="true"}
## Thought: Dealing with variability

Engineers need to...

nice to have a cut-off (threshold). then worst-case assessments are possible.

assessing cracks in welded structures [@BSI2015]

MOTE

:::

This variability can be approximated using probability distributions. These should be considered to be a model 

::: panel-tabset
## Python (using SciPy)

```{python}
stats.norm.fit(data = strength_df['yield'].values, method = 'MLE')

```

## R (using fitdistrplus)

```{R}
fitdist(data = strength_df$yield, distr = 'norm', method = 'mle')

```
:::

These distribution parameters represent those with the highest score (likelihood) of the range considered. If the standard deviation was any higher, the likelihood of any values near the mean would be reduced, and if it was any lower the likelihood of any data at the tails would be reduced. Similarly, if the mean was any higher, the likelihood of any lower values would be reduced. So there is a trade-off here, and maximum likelihood estimates will provide the values that maximise the product of the likelihoods (or the sum of the log-likelihoods) for the data that is being used to fit the distribution.

However, there may often not be a clear maximum likelihood, particularly when estimating distribution parameters from a small dataset. In these cases the *statistical* uncertainty results in many possible values being credible (or having a similar likelihood). These should not be dismissed, and certainly not before there is enough evidence for a model to be confident of it's maximum likelihood estimates.

:::{.callout-note collapse="true"}
## Thought: Collecting Data Reduces Uncertainty
The reason engineers pay for material tests, inspection activities and sensing systems is because the data that they provide can be used to estimate some uncertain quantity of interest. In general, the more data that is available, the less uncertainty will be associated with the prediction. For instance, a linear model with a straight line that approximately goes through two or three points is much less compelling than a straight line that approximately goes through hundreds of points (when the errors are the same).

The uncertainty that is associated with limited amounts of data is often referred to as *statistical* or *epistemic* uncertainty. It is distinct from *aleatory* uncertainty, which is the variability that is inherent in the problem, no matter how many measurements are available.

Since statistical uncertainty can be reduced by collecting more data, there are ways to intelligently identify when it is expected to be worthwhile paying more data, and where the point of diminishing returns is. The example calculations in this document, though presented in various levels of detail, are all based on the premise that collecting data reduces statistical uncertainty, and it is possible to check when this is (and is not) expected to be useful.
:::

Statistical uncertainty in estimates of yield strength will be relatively high when only very few measurements are available. Maximum likelihood estimates will therefore not produce reliable predictions, since they could change significantly after including just a few more tests. It is especially important to understand this variability in cases like this to help distinguish a highly uncertain model with a highly informed model. Failing to do so can mean ac and this distinction is important when they are being used for decision support. 

One method of quantifying variability in a maximum likelihood estimate is to find confidence intervals. Confidence intervals can be obtained by repeating the calculation many times using different samples of the data, and identifying the range within which some proportion of results are contained in. The below code finds the 95% confidence intervals for the maximum likelihood estimate of the mean yield strength, based on the tensile test data in @tbl-strength_data.

Some further detail on confidence intervals can be found in [@Gelman2020], but essentially, the below result should be interpreted as: in repeated experiments, the mean yield strength will lie somewhere within this range 95% of the time. How that fact can be used to support decision making is not clear, so this document considers a more intuitive method of describing this uncertainty.

::: panel-tabset
## Python (using SciPy)

```{python}
yield_data = (strength_df['yield'].values,)

def get_MLE_mean(data):
  return stats.norm.fit(data = data, method = 'MLE')[0]
  
bootstrap_mean = stats.bootstrap(data = yield_data, statistic = get_MLE_mean, vectorized = False, 
confidence_level = 0.95, n_resamples = 1000, method = "basic")

bootstrap_mean.confidence_interval

```

## R, using boot

```{R}
get_MLE_mean <- function(x, id) {fitdist(x[id], distr = 'norm')$estimate[1]}

bootstrap_mean <- strength_df$yield |>
  boot(statistic = get_MLE_mean, R = 1000) |>
  boot.ci(conf = 0.95)

bootstrap_mean$basic |> as_tibble() |> 
  dplyr::select(c(conf, V4, V5)) |>
  rename(lower_bound = V4, upper_bound = V5)

```
:::

# Probabilistic Programming

## Introduction

Probabilistic programming is used to describe a statistical model, and then automate the inference (estimation of the unknown and uncertain parameters) [@Rainforth2017]. Sometimes known as probabilistic machine learning [@Ghahramani2015], inferring unknown parameters, while also accounting for the uncertainty, including statistical uncertainty, is a desirable characteristic of a calculation. One reason for this is because it can be used to demonstrate how additional data can reduce uncertainty, and this can be used as the basis for intelligently collecting data.

There are now many probabilistic programming languages available to engineers, but the one that is used in the examples in this document is `Stan`. The primary justification for this is that it runs a state of the art sampling algorithm, and (unlike many alternatives) it can be used with many other langauges. In these examples, pre- and post-processing of data will be done in `R` and `Python`.

The below functions will be used to load `Stan` files from the Github repository:

::: panel-tabset
## Python

```{python}
#| output: false

def get_stan_model(url):
  new_file, filename = tempfile.mkstemp()
  filename = filename + ".stan"
  
  f = open(filename, "w"); f.write(requests.get(url).text)
  
  return cmdstanpy.CmdStanModel(stan_file = filename)

```

## R

```{R}

get_stan_model <- function(url) {
  url |> url() |>
    readLines() |>
    write_stan_file() |>
    cmdstan_model()
}

```
:::

Firstly, loading the Stan model for quantifying uncertainty in material strength:

::: panel-tabset
## Python

```{python}
#| output: false

strength_model = get_stan_model(url = "https://raw.githubusercontent.com/DomDF/DCE_guidance/main/stan_models/yield_strength_model.stan")

```

## R

```{R}

strength_model <- get_stan_model(url = "https://raw.githubusercontent.com/DomDF/DCE_guidance/main/stan_models/yield_strength_model.stan")

```
:::

The data block in the `Stan` file indicates the data that it is expecting. In `Python` this data is provided to the `Stan` model in the form of a dictionary, and in `R` it is a list.

:::{.callout-note collapse="true"}
## Thought: Data vs. Information
It is widely acknowledged that not all data are equally informative. Engineering data often consists of some indirect measurements of a complex physical phenomena, sometimes in challenging environments. As a result, it will always be associated with some precision, bias and reliability.

It may be necessary to conduct some calibration experiments to quantify these properties, and one such example is provided later in this document. Higher quality (more precise, less biased, more reliable) data will always be at least as useful as lower quality data, and will sometimes be worth paying much more for.

Risk based inspection standards (such as API 580) often acknowledge this difference in quality. A visual inspection is not considered as good as an ultrasonic inspection for damage, and is therefore recommended to be completed more frequently to manage risk. The calculations in this document account for data quality more accurately. Rather than relying on simple heuristics, statistical models are used to relate the information content to the raw data. 
:::


## Sampling Outcomes 

prior predictive sampling - parameters may not have intuitive meaning. Outcomes generally will (particularly amongst domain experts).

prior predictive (or pushword) sampling Recommended practice in Bayesian workflow paper [CITE]. 

Example:

Posterior predictive: pr(S > some value of interest) - ref Pratt, Raiffa, Schlaiffer....
Now we can start combining the results with decision making...

# Supporting Decision Making

## Existing Challenges

Engineering analysis, whether it is a fracture mechanics assessment, a stress analysis, or an environmental forecast is completed with the intention of supporting decision making. And yet, these calculations are often performed seprately from the underlying decision problem, and by a different team.

A consequence 

Value of information analysis is often referred to as *pre-posterior* analysis in engineering textbooks (Benj Cornell, Jordaan, Melchers) and some statistics textbooks too (Berger). 

:::{.callout-note collapse="true"}
## Thought: The Expected Value of Data ...*Before* Collecting it
A key challenge in estimating the expected value of a prospective data collection activity, is that it is performed before the data is available to include in models - the purpose of the analysis is to help decide whether it is worth collecting.

Instead, the method considers all of the information that will be available at the time of making this decision. Engineers will be able to predict, with some (and perhaps a lot of!) uncertainty, what they expect the value to be. They will also know, generally from the contractor, is the quality of data that will be provided. For instance, inspection technologies are calibrated by service providers before they are brought to market, and so along with a quotation, a performance specification can be provided explaining how precise the data will be.

A value of information analysis uses all of this available information and, in the context of the decision problem that the data is intended to support, quantifies on a meaningful, monetary scale, the expected value of the data to the engineer.
:::


### Decision Trees

Linking analysis to decisions is not new : Ref Raiffa, [@Benjamin2014c, @Jordaan2005]...

Convention for square, round and diamond (or sometimes triangular) nodes.

In these books, problems are presented as decision-event trees, such as the example shown in @fig-example_tree.

```{mermaid}
%%| label: fig-example_tree
%%| fig-cap: "Example Structure of a Decision-Event Tree"

flowchart LR
  d1[Decision] -- D1 --> o1((Uncertain outcome, \nD1))
  d1[Decision] -- D2 --> o2((Uncertain outcome,  \nD2))
  
  o1 --O = A --> c1a{Utility, \nO = A, d = D1}
  o1 --O = B --> c1b{Utility, \nO = B, d = D1}
  o2 --O = A--> c2a{Utility, \nO = A, d = D2}
  o2 --O = B--> c2b{Utility, \nO = B, d = D2}
  
```

The expected utility associated with making decision $d1$, $\mathbb{E}\Big[ u(d1) \Big]$ can be calculated as follows:

$$
\mathbb{E}\Big[ u(d1) \Big] = \Pr(O = A) * u(O = A, d = D1) + \Pr(O = B) * u(O = B, d = D1)
$$

Here, the utilities (or costs) are being weighted by the probabilities that they will occur. The expected utility associated with making decision $D2$ can be calculated in the same way, and the expected optimal decision will be the option that has the highest expected utility, or lowest expected cost.

<!-- Formally, ... -->

<!-- $$ -->
<!-- a^{*} = \arg \max_{a \in A} E \Big[ u(a, \theta) \Big] -->
<!-- $$ -->

<!-- On the basis that the expected optimal action is selected, the expected utility is $E \Big[ u(a^{*}, \theta) \Big]$. -->

### Graphical Models (Influence Diagrams)

Approximating real engineering systems using decision-event trees will require many more nodes than those presented in @fig-example_tree. This would lead to very large diagrams. Consequenlty, they are often represented more concisely, using *influence diagrams*.

Influence diagrams do not show every possible event path graphically (rather, these are stored as tables behind each node.) An influence diagram representation of @fig-example_tree is shown in @fig-example_id.

```{mermaid}
%%| label: fig-example_id
%%| fig-cap: "Example Structure of an Influence Diagram"

flowchart LR
  d1[Decision] --> o1((Uncertain \noutcome))
  d1 --> c1{Utility}
  o1 --> c1

```

The arrows in an influence diagram are causal (like in a Bayesian network). The arrows in @fig-example_id imply that the selected decision influences the uncertain outcome, and that the utility is conditional on both of these parameters. Drawing a useful influence diagram therefore requires some knowledge of the system that it is designed to represent.

:::{.callout-note collapse="true"}
## Thought: Drawing Upon Engineering Know-How
Drawing a graphical model can simply follow on, from a description of an engineering system.

If tasked with describing a corrosion protection system as an influence diagram, to help identify whether a data collection activity is expected to be worthwhile, it would be recommended to consult a corrosion specialist.

If a pipeline integrity engineer provided the following, basic information about the protection system for external corrosion of a buried pipeline:

 - The rate of corrosion will depend on the type of soil in which the pipeline is buried, amongst other factors.
 - The primary external protective system is a protective coating, which provides a physical barrier between the steel and the soil. 
 - The pipeline is also protected by a Cathodic Protection (CP) system, which helps protect any exposed steel by ensuring it is sufficiently cathodic to prevent the oxidising corrosion reaction.
 - Insufficient current from the CP system will mean that exposed steel (for example at locations of coating damage) will corrode. Excessive current can lead to disbondment of the coating from pipeline surface, meaning there may be a corrosive environment between the pipeline and the coating, which is shielded from any CP protection.
 - A Direct Current Voltage Gradient (DCVG) survey, can provide us with some information about the presence of damage in the coating.

This information can be shown graphically, using the influence diagram shown in @fig-id_cr. This representation allows for information on any of the uncertain parameter can be propagated through the network. It also describes how decision/actions can affect specific parameters in a model (and therefore also in the outcomes of interest, i.e. external corrosion rate). 

Using a model to predict how a system will respond to various interventions, as facilitated by influence diagrams, is often described as the goal of *digital twins*.

```{mermaid}
%%| label: fig-id_cr
%%| fig-cap: "Influence Diagram Showing Factors Affecting Corrosion Rate of a Buried Pipeline"

flowchart LR
 
  dcoat[Complete \nDCVG Survey?] --> coat((Coating \nCondition))
  cp((CP \nPerformance)) --> corr((Corrosion \nRate))
  coat --> corr
  dcoat --> ccoat{DCVG \n Costs}
  
  cp --> ccp{CP costs}
  cp --> coat
  
  soil((Soil Type)) --> corr
  corr --> ccorr{Corrosion \ncosts}
  
```

:::

## Example Calculations

### Crop Sales Forecasting

...

### Energy Use in Buildings

Consider the problem of heating a residential building. Inefficiencies in ageing heaters can result in increased electricity costs to deliver energy requirements. This performance degradtio can be mitigated by maintenance work.

The problem is described by the influence diagram in @fig-prior_id_heat_pumps. Here, heat pump parameters, $\alpha$ and $\beta$ describe the degradation and improvements to the performance of a heat pump, respectively.

Heat pump efficiency, characterised by a Seasonal Performance Factor (SPF), is calculated based on how the system degrades with age (using $\alpha$), and how maintenance activites can improve, or restore, the performance (using $\beta$).

```{mermaid}
%%| label: fig-prior_id_heat_pumps
%%| fig-cap: "Influence Diagram for Identifying Expected Optimal Maintenance Schedule of Heating System"

flowchart LR
  d2[Maintenance \nfrequency] --> o2((SPF))

  o0((Heating \nload)) --> o1((SPF))
  o2(("#946;")) --> o1((SPF))
  o3(("#945;")) --> o1((SPF))

  o1 --> c1{Electricity \ncost}
  d2 --> c2{Maintenance \n cost}

```

The model that will be used is as follows:

$$
\beta = \frac{\beta_{A} * N_{m}^\gamma}{\beta_{B} + N_{m}^{\gamma}}
$$
$$
\alpha \sim N(\mu = 1.6 \times 10^{-2}, \sigma = 1.6 \times 10^{-3})
$$
Where: $\beta_{A} = 0.073$, $\beta_{B} = 11.242$, and $\gamma = 1.307$. The cost of a heat pump is taken to be £$110472.50$, and the cost of  maintenance is equal to $1$% of this cost, multiplied by the number of activities, $N_{m}$.

The cost of electricity for the system, $C_{e}$ is calculated, using the heating load in kWh, $L_{H}$, the price per kWh, in GBP $p_{e}$, and the SPF:
$$
C_{e} = \frac{L_{H} \times p_{e}}{SPF}
$$

The inputs for the problem are defined below. These result in a decreasing efficacy of maintenance activities, when many are scheduled, see @fig-n_maint_beta.

::: {#chunk_load_packages .panel-tabset}

## Python (using SciPy)

```{python}
def get_beta(n_maintenance):
  a = 0.07349026; b = 11.24172755; gamma = 1.30736477
  return (a * n_maintenance ** gamma) / (b + n_maintenance ** gamma)

n_samples = 10**3; alpha = stats.norm.rvs(loc = 1.6*10**-2, scale = 1.6 * 10**-3)

SPF_inital = 3

max_n_maint = 12; n_maint = np.arange(start = 0, stop = max_n_maint + 1, step = 1)

heat_pump_cost_GBP = 441890/4; heating_load_kWh = 1753914; GBP_per_kWh = 0.51
```

## R

```{R}
get_beta <- function(n_maintenance){
  a <- 0.07349026; b <- 11.24172755; gamma <- 1.30736477
  # a <- 1/4; gamma <- 2; b <- 10
  (a * n_maintenance^gamma) / (b + n_maintenance^gamma)
}

n_samples <- 1e3; alpha <- rnorm(n = n_samples, mean = 1.6e-2, sd = 1.6e-3)

SPF_initial <- 3

max_n_maint <- 12; n_maint <- seq(from = 0, to = max_n_maint, by = 1)

heat_pump_cost_GBP <- 441890/4; heating_load_kWh <- 1753914; GBP_per_kWh <- 0.51
```

:::

```{R}
#| echo: false
#| label: fig-n_maint_beta
#| fig-cap: "Effect of Increasing Number of Scheduled Maintenance Activities on Heat Pump Performance Improvement Factor"

tibble(x = n_maint) |>
  mutate(beta = get_beta(n_maintenance = x)) |>
  ggplot(mapping = aes(x, beta))+
  geom_point(shape = 1, size = 1)+
  geom_line(alpha = 1/4)+
  scale_x_continuous(name = 'Number of scheduled maintenance activities per year')+
  scale_y_continuous(name = expression(beta))+
  ggthemes::theme_base(base_size = 12, base_family = "Atkinson Hyperlegible")+
  theme(plot.background = element_rect(color = NA),
        legend.title = element_blank(), legend.position = 'top')
```

As shown in @fig-prior_heat_pump_decision, increasing the number of maintenance activities initially decreases the total cost (sum of electricity and maintenance costs), before no longer expecting to be worthwhile.

::: {#chunk_load_packages .panel-tabset}

## Python (using SciPy)

```{python}
def prior_decision_df(n_maint_range):
  return 0

```

## R (using purrr)

```{R}
prior_decision_df <- function(n_maint_range){
  
  tibble(n_maint = n_maint_range, alpha = list(alpha)) |>
  mutate(beta = get_beta(n_maintenance = n_maint),
         SPF = SPF_initial,
         maint_cost = n_maint * heat_pump_cost_GBP * 1e-2,
         SPF_coeff = map2(.x = alpha, .y = beta,
                          .f = function(.x, .y){(1 - .x) * (1 + .y)}),
         SPF = map2(.x = SPF, .y = SPF_coeff,
                    .f = function(.x, .y){lag(x = .x, n = 1, default = SPF_initial) * .y}),
         elec_cost = map(.x = SPF,
                         .f = function(.x){GBP_per_kWh * heating_load_kWh / .x}),
         total_cost = map2(.x = elec_cost, .y = maint_cost,
                           .f = function(.x, .y){ .x + .y})) |> 
    group_by(n_maint) |>
    mutate(exp_cost = mean(unlist(total_cost))) |>
    ungroup()

  }
```

:::

```{R}
#| echo: false
#| label: fig-prior_heat_pump_decision
#| fig-cap: "Identification of Number of Expected Optimal Heat Pump Maintenance Activities"

prior_df <- prior_decision_df(n_maint_range = n_maint)

ggplot(data = prior_df |>  
              tidyr::unnest(cols = total_cost) |>
              mutate(id = rep(x = seq(from = 1, to = n_samples, by = 1), times = max_n_maint + 1)), 
       mapping = aes(x = n_maint, y = total_cost))+
  geom_line(mapping = aes(group = id), alpha = 1/50)+
  scale_x_continuous(name = 'Number of scheduled maintenance activities per year', 
                     breaks = seq(from = 0, to = 12, by = 1))+
  scale_y_continuous(name = 'Total (annual) operational costs, £', labels = scales::comma)+
  geom_vline(lty = 2, alpha = 1/2, 
             mapping = aes(color = 'Expected minimum cost',
                           xintercept = prior_df[which.min(prior_df$exp_cost),]$n_maint))+
  ggthemes::theme_base(base_size = 12, base_family = "Atkinson Hyperlegible")+
  theme(plot.background = element_rect(color = NA),
        legend.title = element_blank(), legend.position = 'top')

```

The expected optimal number of maintenance activities to schedule (that which is associated with the lowest expected cost) is `r prior_df[which.min(prior_df$exp_cost),]$n_maint`, with an associated expected cost of £`r prior_df[which.min(prior_df$exp_cost),]$exp_cost |> signif(digits = 4)`.

How can a smart meter assist in solving this decision problem? As shown in @fig-prepost_id_heat_pumps, the data that a smart meter provides can be used to reduce uncertainty about the degradation parameter, $\alpha$.

```{mermaid}
%%| label: fig-prepost_id_heat_pumps
%%| fig-cap: "Influence Diagram for Estimating the Expected Value of Smart Meters"

flowchart LR
  d2[Maintenance \nfrequency] --> o2((SPF))

  o0((Heating \nload)) --> o1((SPF))
  o2(("#946;")) --> o1((SPF))
  o3(("#945;")) --> o1((SPF))

  o1 --> c1{Electricity \ncost}
  d2 --> c2{Maintenance \ncost}

  d3[Smart \nmeter] --> c3{Meter \ncost}
  d3 --> o3

```

::: {#chunk_load_packages .panel-tabset}

## Python (using SciPy)

```{python}
def prepost_decision_df(n_maint_range):
  return 0

```

## R (using purrr)

```{R}

prepost_decision_df <- function(n_maint_range){
  
  tibble(alpha = alpha, n_maint = list(n_maint_range)) |>
  mutate(beta = map(.x = n_maint, 
                    .f = function(.x){get_beta(.x)}),
         SPF = SPF_initial,
         maint_cost = map(.x = n_maint,
                          .f = function(.x) {.x * heat_pump_cost_GBP * 1e-2}),
         SPF_coeff = map2(.x = alpha, .y = beta,
                          .f = function(.x, .y){(1 - .x) * (1 + .y)}),
         SPF = map2(.x = SPF, .y = SPF_coeff,
                    .f = function(.x, .y){
                      lag(x = .x, n = 1, default = SPF_initial) * .y}),
         elec_cost = map(.x = SPF,
                         .f = function(.x){GBP_per_kWh * heating_load_kWh / .x}),
         total_cost = map2(.x = elec_cost, .y = maint_cost,
                           .f = function(.x, .y){ .x + .y})) |>
    mutate(exp_cost = map(.x = total_cost, 
                          .f = function(.x) {min(unlist(.x))}),
           exp_cost = unlist(exp_cost))
}
```

:::

```{R}
#| echo: false

prior_df <- tibble(alpha_sd = double(), ec_pr = double(), a_pr = integer())
prepost_df <- tibble(alpha_sd = double(), ec_prepost = double())

for (alpha_sd in c(0.01, 0.05, 0.1, 0.25)){
  
  alpha <- rnorm(n = n_samples, mean = 1/2, sd = alpha_sd)
  
  df_pr <- prior_decision_df(n_maint_range = n_maint)
  
  prior_df <- bind_rows(prior_df, 
                        tibble(alpha_sd = alpha_sd, ec_pr = df_pr[which.min(df_pr$exp_cost),]$exp_cost, a_pr = df_pr[which.min(df_pr$exp_cost),]$n_maint))
    
  df_pp <- prepost_decision_df(n_maint_range = n_maint)
  
  prepost_df <- bind_rows(prepost_df, 
                          tibble(alpha_sd = alpha_sd, ec_prepost = df_pp$exp_cost |> mean()))
  
}
  
VoPI_df <- prior_df |>
  left_join(y = prepost_df, by = 'alpha_sd') |>
  mutate(VoPI = ec_pr - ec_prepost)

```

```{R}
#| echo: false
#| label: fig-voi_smart_meter_alpha_sensitivity
#| fig-cap: "Effect of Prior Uncertainty on Expected Value of Smart Meter Data"

ggplot(data = VoPI_df, mapping = aes(x = alpha_sd, y = VoPI))+
  geom_point(shape = 1, size = 1) + geom_line(alpha = 1/4)+
  geom_text(mapping = aes(label = paste("£", VoPI |> signif(digits = 3))), 
            family = "Atkinson Hyperlegible", size = 3, vjust = -1)+
  scale_x_continuous(name = expression(paste('Uncertainty (std. dev.) in prior model of ', alpha)))+
  scale_y_continuous(name = 'Expected value of smart meter data, £', labels = scales::comma, limits = c(0, VoPI_df$VoPI |> max() + 3e3))+
  ggthemes::theme_base(base_size = 12, base_family = "Atkinson Hyperlegible")+
  theme(plot.background = element_rect(color = NA))
```

### Design of Energy Generation Asset Portfolio 

In designing an energy system for...

::: {#chunk_load_packages .panel-tabset}

## Julia (using Convex)

```{julia}
#| output: false

assets = ["solar", "offshore", "nearshore"]

α = 10; β = 2; γ = 1 * 0.1 * 24 * 365

```

:::

```{julia}
α

```

```{mermaid}
%%| label: fig-prepost_id_energy_portfolio
%%| fig-cap: "Influence Diagram for Estimating the Expected Value of an Energy Forecasting Model"

flowchart LR
  d0[System \ndesign] --> ons((Nearshore wind \ngeneration))
  d0[System \ndesign] --> oos((Offshore wind \ngeneration))
  d0[System \ndesign] --> os((Solar \ngeneration))

  ons --> c_s
  oos --> c_s
  os --> c_s 

  d1[Forecast \nmodel] --> ec((Energy \nprices))
  ec --> c_s{Implementation \ncosts}

  d1 --> c_m{Modelling \ncosts}

```


### Identifying a Data Collection Plan for a Bridge

<!-- Structural reliability is concerned with... -->

<!-- It is an established field of engineering, and reliability analysis serves as the technical justification for some of the lower level guidance in many engineering codes and standards. -->

<!-- The probability will be dependent on the mdoel assumptions - and this has lead to multiple interpretations of SRA results. In many cases, it is recommended that these probabilities shoul;d only be considered... -->

<!-- Some of the topics discussed in this document help address perceived challenges with SRA. For instance, the *tail sensitivity problem*... -->

<!-- Consider the challenge of the integrity management of a rail bridge. Here, there are  -->

# Recommendations

...

<!-- ## Probabilistic Models -->

<!-- ## Neural Networks -->

<!-- :::{.callout-note collapse="true"} -->
<!-- ## Thought: Learning a Model -->

<!-- ::: -->

<!-- ## Engineering Judgement -->


